{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import database\n",
    "database = pickle.load( open( \"Mod_AnyT_DB.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diameters = []\n",
    "y_res_index = []\n",
    "for i in range(len(database)):\n",
    "    x_diameters.append(database.loc[i]['Diams'])\n",
    "    y_res_index.append(database.loc[i]['avgPrPa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 1000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_diameters, y_res_index, random_state=1, test_size=0.2 )\n",
    "print(len(X_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = torch.tensor(X_train, dtype = torch.float32), torch.tensor(X_test, dtype = torch.float32), torch.tensor(y_train, dtype = torch.float32), torch.tensor(y_test, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, h_sizes, out_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_linear = nn.ModuleList()\n",
    "        self.hidden_dropout = nn.ModuleList()\n",
    "        self.h_len = len(h_sizes)\n",
    "        \n",
    "        #Add initial layer\n",
    "        self.hidden_linear.append(nn.Linear(in_features, h_sizes[0]))\n",
    "        self.hidden_dropout.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        #Add hidden layers        \n",
    "        for k in range(len(h_sizes)-1):\n",
    "            self.hidden_linear.append(nn.Linear(h_sizes[k], h_sizes[k+1]))\n",
    "            self.hidden_dropout.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        #Add output layer\n",
    "        self.hidden_linear.append(nn.Linear(h_sizes[-1], out_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for i in range(len(self.hidden_dropout)):\n",
    "            y = self.hidden_linear[i](y)\n",
    "            y = self.hidden_dropout[i](y)\n",
    "            y = torch.tanh(y) #F.relu(y)\n",
    "        \n",
    "        y = self.hidden_linear[-1](y)\n",
    "        y = torch.tanh(y)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(40, [50, 50, 50], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp(torch.rand(1,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "batch_size = 50\n",
    "lr = 0.001\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_X_train = torch.utils.data.DataLoader(\n",
    "    X_train,\n",
    "    shuffle=False,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "data_loader_y_train = torch.utils.data.DataLoader(\n",
    "    y_train,\n",
    "    shuffle = False,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay, amsgrad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  total loss   tensor(0.4967, grad_fn=<AddBackward0>)\n",
      "epoch  50  total loss   tensor(0.4836, grad_fn=<AddBackward0>)\n",
      "epoch  100  total loss   tensor(0.4703, grad_fn=<AddBackward0>)\n",
      "epoch  150  total loss   tensor(0.4641, grad_fn=<AddBackward0>)\n",
      "epoch  200  total loss   tensor(0.4485, grad_fn=<AddBackward0>)\n",
      "epoch  250  total loss   tensor(0.4871, grad_fn=<AddBackward0>)\n",
      "epoch  300  total loss   tensor(0.4602, grad_fn=<AddBackward0>)\n",
      "epoch  350  total loss   tensor(0.4325, grad_fn=<AddBackward0>)\n",
      "epoch  400  total loss   tensor(0.4589, grad_fn=<AddBackward0>)\n",
      "epoch  450  total loss   tensor(0.4454, grad_fn=<AddBackward0>)\n",
      "epoch  500  total loss   tensor(0.4417, grad_fn=<AddBackward0>)\n",
      "epoch  550  total loss   tensor(0.4330, grad_fn=<AddBackward0>)\n",
      "epoch  600  total loss   tensor(0.4351, grad_fn=<AddBackward0>)\n",
      "epoch  650  total loss   tensor(0.4341, grad_fn=<AddBackward0>)\n",
      "epoch  700  total loss   tensor(0.4340, grad_fn=<AddBackward0>)\n",
      "epoch  750  total loss   tensor(0.4166, grad_fn=<AddBackward0>)\n",
      "epoch  799  total loss   tensor(0.4243, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp.train()\n",
    "f_loss = nn.MSELoss() #(preds-y_train[i])**2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss= 0\n",
    "#     total_correct =0\n",
    "    \n",
    "    iter_y_train = iter(data_loader_y_train)\n",
    "    iter_X_train = iter(data_loader_X_train)\n",
    "    \n",
    "    #Data\n",
    "    for batch in iter_X_train:\n",
    "        #Prediction\n",
    "        preds = mlp(batch)\n",
    "\n",
    "        #Calculate the loss\n",
    "        loss = f_loss(preds.reshape(batch_size,1), next(iter_y_train).reshape(batch_size,1))\n",
    "        \n",
    "        total_loss += loss\n",
    "    \n",
    "    #Backpropagate\n",
    "    optimizer.zero_grad() #To avoid adding up gradients\n",
    "    total_loss.backward() #calculate gradients\n",
    "\n",
    "    #Optimizer step\n",
    "    optimizer.step() #Update weights\n",
    "        \n",
    "    if epoch%50 == 0:\n",
    "        print(\"epoch \", epoch, \" total loss  \", total_loss )\n",
    "    \n",
    "    if epoch == num_epochs-1:\n",
    "        print(\"epoch \", epoch, \" total loss  \", total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4542],\n",
       "        [ 0.2443],\n",
       "        [ 0.3621],\n",
       "        [ 0.4023],\n",
       "        [ 0.2997],\n",
       "        [ 0.3387],\n",
       "        [ 0.4273],\n",
       "        [ 0.3234],\n",
       "        [-0.0033]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(X_test[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4086,  0.0567,  0.3450,  0.4014,  0.3403,  0.3413,  0.4215,  0.4311,\n",
       "        -0.1600])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6751711843709476"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R squared \n",
    "r2_score( y_train.detach().numpy() , mlp(X_train).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6663847495652011"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R squared\n",
    "r2_score( y_test.detach().numpy() , mlp(X_test).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
